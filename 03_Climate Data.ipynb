{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data specs #\n",
    "data_header_col_specs = [(0,11),(11,15),(15,17),(17,21)]\n",
    "data_header_names = [\"ID\",\"YEAR\",\"MONTH\",\"ELEMENT\"]\n",
    "data_header_dtypes = {\"ID\":str, \"YEAR\":int, \"MONTH\":int, \"ELEMENT\":str}\n",
    "\n",
    "data_col_names = [[\"VALUE\" + str(i+1),\n",
    "                  \"MFLAG\" + str(i+1),\n",
    "                  \"QFLAG\" + str(i+1),\n",
    "                  \"SFLAG\" + str(i+1)]\n",
    "                 for i in range(31)]\n",
    "data_col_names = sum(data_col_names, [])\n",
    "\n",
    "data_col_specs = [[\n",
    "    (21 + i * 8, 26 + i * 8),\n",
    "    (26 + i * 8, 27 + i * 8),\n",
    "    (27 + i * 8, 28 + i * 8),\n",
    "    (28 + i * 8, 29 + i * 8)]\n",
    "    for i in range(31)]\n",
    "data_col_specs = sum(data_col_specs, [])\n",
    "\n",
    "data_col_dtypes = [{\n",
    "    \"VALUE\" + str(i + 1): int,\n",
    "    \"MFLAG\" + str(i + 1): str,\n",
    "    \"QFLAG\" + str(i + 1): str,\n",
    "    \"SFLAG\" + str(i + 1): str}\n",
    "    for i in range(31)]\n",
    "data_header_dtypes.update({k: v for d in data_col_dtypes for k, v in d.items()})\n",
    "\n",
    "vals = [\"VALUE\" + str(i+1) for i in range(31)]\n",
    "mfs = [\"MFLAG\" + str(i+1) for i in range(31)]\n",
    "qfs = [\"QFLAG\" + str(i+1) for i in range(31)]\n",
    "sfs = [\"SFLAG\" + str(i+1) for i in range(31)]\n",
    "day = [i+1 for i in range(31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ghcn_data_file(filename=\"USC00011084.dly\"):\n",
    "    \"\"\"Reads in all data from a GHCN .dly data file\n",
    "\n",
    "    :param filename: path to file\n",
    "    :param variables: list of variables to include in output dataframe\n",
    "        e.g. ['TMAX', 'TMIN', 'PRCP']\n",
    "    :param include_flags: Whether to include data quality flags in the final output\n",
    "    :returns: Pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_fwf(\n",
    "        filename,\n",
    "        colspecs=data_header_col_specs + data_col_specs,\n",
    "        names=data_header_names + data_col_names,\n",
    "        dtype=data_header_dtypes\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all files from directory and importing\n",
    "path = '/Users/hkelderman/DataScience/Bootcamp/Capstone/Climate Data/ghcnd_hcn'\n",
    "all_files = glob.glob(path + \"/*.dly\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = read_ghcn_data_file(filename)\n",
    "    li.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#concatonating all all climate dataframes into one\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9663559 entries, 0 to 9663558\n",
      "Columns: 128 entries, ID to SFLAG31\n",
      "dtypes: int64(33), object(95)\n",
      "memory usage: 9.2+ GB\n"
     ]
    }
   ],
   "source": [
    "frame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Saving Needed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>ELEMENT</th>\n",
       "      <th>VALUE1</th>\n",
       "      <th>MFLAG1</th>\n",
       "      <th>QFLAG1</th>\n",
       "      <th>SFLAG1</th>\n",
       "      <th>VALUE2</th>\n",
       "      <th>MFLAG2</th>\n",
       "      <th>...</th>\n",
       "      <th>QFLAG29</th>\n",
       "      <th>SFLAG29</th>\n",
       "      <th>VALUE30</th>\n",
       "      <th>MFLAG30</th>\n",
       "      <th>QFLAG30</th>\n",
       "      <th>SFLAG30</th>\n",
       "      <th>VALUE31</th>\n",
       "      <th>MFLAG31</th>\n",
       "      <th>QFLAG31</th>\n",
       "      <th>SFLAG31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00137147</td>\n",
       "      <td>1893</td>\n",
       "      <td>4</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00137147</td>\n",
       "      <td>1893</td>\n",
       "      <td>4</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00137147</td>\n",
       "      <td>1893</td>\n",
       "      <td>4</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00137147</td>\n",
       "      <td>1893</td>\n",
       "      <td>4</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00137147</td>\n",
       "      <td>1893</td>\n",
       "      <td>4</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  YEAR  MONTH ELEMENT  VALUE1 MFLAG1 QFLAG1 SFLAG1  VALUE2  \\\n",
       "0  USC00137147  1893      4    TMAX   -9999    NaN    NaN    NaN   -9999   \n",
       "1  USC00137147  1893      4    TMIN   -9999    NaN    NaN    NaN   -9999   \n",
       "2  USC00137147  1893      4    PRCP   -9999    NaN    NaN    NaN   -9999   \n",
       "3  USC00137147  1893      4    SNOW   -9999    NaN    NaN    NaN   -9999   \n",
       "4  USC00137147  1893      4    SNWD   -9999    NaN    NaN    NaN   -9999   \n",
       "\n",
       "  MFLAG2  ... QFLAG29 SFLAG29  VALUE30 MFLAG30 QFLAG30 SFLAG30  VALUE31  \\\n",
       "0    NaN  ...     NaN       0       67     NaN     NaN       0    -9999   \n",
       "1    NaN  ...     NaN       0       22     NaN     NaN       0    -9999   \n",
       "2    NaN  ...     NaN       0        0       T     NaN       0    -9999   \n",
       "3    NaN  ...     NaN       0        0     NaN     NaN       0    -9999   \n",
       "4    NaN  ...     NaN     NaN    -9999     NaN     NaN     NaN    -9999   \n",
       "\n",
       "  MFLAG31 QFLAG31 SFLAG31  \n",
       "0     NaN     NaN     NaN  \n",
       "1     NaN     NaN     NaN  \n",
       "2     NaN     NaN     NaN  \n",
       "3     NaN     NaN     NaN  \n",
       "4     NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = frame[frame.YEAR >= 1984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3486814 entries, 6899 to 9663558\n",
      "Columns: 128 entries, ID to SFLAG31\n",
      "dtypes: int64(33), object(95)\n",
      "memory usage: 3.4+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into more managable dataframes\n",
    "df1 = df.iloc[0:871704]\n",
    "df2 = df.iloc[871704:1743408]\n",
    "df3 = df.iloc[1743408:2615112]\n",
    "df4 = df.iloc[2615112:3486814]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then saving the split dataframes to csvs, so as to not have to go through the joining process again\n",
    "df1.to_csv('Climate_Data_1.csv')\n",
    "df2.to_csv('Climate_Data_2.csv')\n",
    "df3.to_csv('Climate_Data_3.csv')\n",
    "df4.to_csv('Climate_Data_4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in CSVs after Kernal restart\n",
    "df1 = pd.read_csv('Climate_Data_1.csv')\n",
    "df2 = pd.read_csv('Climate_Data_2.csv')\n",
    "df3 = pd.read_csv('Climate_Data_3.csv')\n",
    "df4 = pd.read_csv('Climate_Data_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists for climate data cleaning\n",
    "elements_to_keep = ['TMAX', 'TMIN', 'PRCP', 'AWND']\n",
    "data_header_names = [\"ID\",\"YEAR\",\"MONTH\",\"ELEMENT\"]\n",
    "vals = [\"VALUE\" + str(i+1) for i in range(31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_clean_1(df):\n",
    "    \"\"\"This function will clean each dataframe of daily climate data. It will remove elements we don't care about,\n",
    "    melt the dataframe so that we have one row per day of values, and create a date column we may use further on.\n",
    "    \n",
    "    Attributes:\n",
    "    df - dataframe of daily climate data\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[df.ELEMENT.isin(elements_to_keep)] #remove excess elements\n",
    "    df_2 = pd.melt(df, id_vars=data_header_names, value_vars=vals, var_name='Day_col') #elongate dataframe\n",
    "    df_2['Day'] = df_2['Day_col'].str.replace('VALUE', '') #create day column based on integer in 'Day_col'\n",
    "    df_2[['YEAR', 'MONTH', 'Day']] = df_2[['YEAR', 'MONTH', 'Day']].astype(str) #convert each piece of date to str\n",
    "    df_2['Day_clean'] = np.where(len(df_2['Day']) == 2, df_2['Day'], df_2['Day'].apply(lambda x: x.rjust(2, '0'))) #pad zeros to day\n",
    "    df_2['Month_clean'] = np.where(len(df_2['MONTH']) == 2, df_2['MONTH'], df_2['MONTH'].apply(lambda x: x.rjust(2, '0'))) #pad zeros to month\n",
    "    df_2['Date'] = df_2[['YEAR', 'Month_clean', 'Day_clean']].apply(lambda x: '-'.join(x), axis=1) #join to create a date col\n",
    "    df_2['Date'] = pd.to_datetime(df_2['Date'], format='%Y-%m-%d', errors='coerce')\n",
    "    \n",
    "    df_2.drop(columns=['Day_col', 'YEAR', 'MONTH', 'Day', 'Day_clean', 'Month_clean'], inplace=True) #remove excess columns\n",
    "    df_2['value'] = np.where((df_2['value'] == -9999), np.nan, df_2['value']) #replacing -9999 with NaN, so as not to interfere with future calculations\n",
    "    df_2.dropna(subset=['Date'], inplace=True)\n",
    "    \n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_clean_2(df):\n",
    "    \"\"\"Now that we've removed the excess elements, created a date column, and taken care of the missing values,\n",
    "    we will use this function to spread the dataframe back out based on the elements, as well as calculate the true\n",
    "    value for each element based on the text found in the NOAA ReadMe file. We will also use this function to create\n",
    "    various rolling averages for each date.\n",
    "    \n",
    "    Attributes:\n",
    "    df - dataframe of daily climate data cleaned through round one\n",
    "    \"\"\"\n",
    "    #spread data out based on ELEMENT column\n",
    "    df = df.pivot(index=['ID', 'Date'], columns='ELEMENT', values='value')\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    #Temperature is reported tenths of degrees C, so this will convert the temperatures to degrees C\n",
    "    df['max_temp_C'] = df['TMAX'] / 10\n",
    "    df['min_temp_C'] = df['TMIN'] / 10\n",
    "\n",
    "    #Precipitation is reported in tenths of mm, so this will convert the precipiation to cm\n",
    "    df['precipitation_cm'] = df['PRCP'] / 100\n",
    "\n",
    "    #Wind speed is measured in tenths of a meter/s, so this will convert the speed to m/s\n",
    "    df['wind_speed_ms'] = df['AWND'] / 10\n",
    "    \n",
    "    #drop extra columns\n",
    "    df.drop(columns=['AWND', 'PRCP', 'TMAX', 'TMIN'], inplace=True)\n",
    "    \n",
    "    #creating rolling averages\n",
    "    cols = ['max_temp_C', 'min_temp_C', 'precipitation_cm', 'wind_speed_ms']\n",
    "    periods = [30, 90, 180, 365]\n",
    "    period_str = ['30', '90', '180', '365']\n",
    "    \n",
    "    for col in cols:\n",
    "        for i in range(len(periods)):\n",
    "            df['{}_{}'.format(col,period_str[i])] = df.groupby('ID')[col].transform(lambda x: x.rolling(periods[i], 1).mean())\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_1 = climate_clean_1(df1)\n",
    "daily_2 = climate_clean_1(df2)\n",
    "daily_3 = climate_clean_1(df3)\n",
    "daily_4 = climate_clean_1(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_1 = climate_clean_2(daily_1)\n",
    "daily_2 = climate_clean_2(daily_2)\n",
    "daily_3 = climate_clean_2(daily_3)\n",
    "daily_4 = climate_clean_2(daily_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining\n",
    "daily_weather = pd.concat([daily_1, daily_2, daily_3, daily_4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting only dates of fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_dates = pd.read_csv('Fire_dates.csv')\n",
    "dates = list(fire_dates.IG_Date.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14603596"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(daily_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = daily_weather[daily_weather['Date'].isin(dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7211938"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join with meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_reg = pd.read_csv('Monitor_by_Region.csv')\n",
    "mon_reg.drop(columns='Unnamed: 0', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_mon = pd.merge(weather, mon_reg, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>max_temp_C</th>\n",
       "      <th>min_temp_C</th>\n",
       "      <th>precipitation_cm</th>\n",
       "      <th>wind_speed_ms</th>\n",
       "      <th>max_temp_C_30</th>\n",
       "      <th>max_temp_C_90</th>\n",
       "      <th>max_temp_C_180</th>\n",
       "      <th>max_temp_C_365</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speed_ms_90</th>\n",
       "      <th>wind_speed_ms_180</th>\n",
       "      <th>wind_speed_ms_365</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NA_L3NAME</th>\n",
       "      <th>NA_L2NAME</th>\n",
       "      <th>NA_L1NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00013816</td>\n",
       "      <td>1984-02-21</td>\n",
       "      <td>15.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.493333</td>\n",
       "      <td>14.515385</td>\n",
       "      <td>14.515385</td>\n",
       "      <td>14.515385</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.8814</td>\n",
       "      <td>-86.2503</td>\n",
       "      <td>132.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Southeastern Plains</td>\n",
       "      <td>SOUTHEASTERN USA PLAINS</td>\n",
       "      <td>EASTERN TEMPERATE FORESTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00013816</td>\n",
       "      <td>1984-03-29</td>\n",
       "      <td>27.2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.923333</td>\n",
       "      <td>16.564045</td>\n",
       "      <td>16.564045</td>\n",
       "      <td>16.564045</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.8814</td>\n",
       "      <td>-86.2503</td>\n",
       "      <td>132.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Southeastern Plains</td>\n",
       "      <td>SOUTHEASTERN USA PLAINS</td>\n",
       "      <td>EASTERN TEMPERATE FORESTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00013816</td>\n",
       "      <td>1984-04-18</td>\n",
       "      <td>18.3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.730000</td>\n",
       "      <td>18.252222</td>\n",
       "      <td>17.354128</td>\n",
       "      <td>17.354128</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.8814</td>\n",
       "      <td>-86.2503</td>\n",
       "      <td>132.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Southeastern Plains</td>\n",
       "      <td>SOUTHEASTERN USA PLAINS</td>\n",
       "      <td>EASTERN TEMPERATE FORESTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00013816</td>\n",
       "      <td>1984-04-20</td>\n",
       "      <td>24.4</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.766667</td>\n",
       "      <td>18.695556</td>\n",
       "      <td>17.461261</td>\n",
       "      <td>17.461261</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.8814</td>\n",
       "      <td>-86.2503</td>\n",
       "      <td>132.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Southeastern Plains</td>\n",
       "      <td>SOUTHEASTERN USA PLAINS</td>\n",
       "      <td>EASTERN TEMPERATE FORESTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00013816</td>\n",
       "      <td>1984-04-21</td>\n",
       "      <td>29.4</td>\n",
       "      <td>17.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.116667</td>\n",
       "      <td>18.973333</td>\n",
       "      <td>17.567857</td>\n",
       "      <td>17.567857</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.8814</td>\n",
       "      <td>-86.2503</td>\n",
       "      <td>132.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Southeastern Plains</td>\n",
       "      <td>SOUTHEASTERN USA PLAINS</td>\n",
       "      <td>EASTERN TEMPERATE FORESTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID       Date  max_temp_C  min_temp_C  precipitation_cm  \\\n",
       "0  USC00013816 1984-02-21        15.6         6.1              0.05   \n",
       "1  USC00013816 1984-03-29        27.2         7.8              0.05   \n",
       "2  USC00013816 1984-04-18        18.3         6.7              0.00   \n",
       "3  USC00013816 1984-04-20        24.4        15.6              0.23   \n",
       "4  USC00013816 1984-04-21        29.4        17.2              0.00   \n",
       "\n",
       "   wind_speed_ms  max_temp_C_30  max_temp_C_90  max_temp_C_180  \\\n",
       "0            NaN      16.493333      14.515385       14.515385   \n",
       "1            NaN      19.923333      16.564045       16.564045   \n",
       "2            NaN      21.730000      18.252222       17.354128   \n",
       "3            NaN      21.766667      18.695556       17.461261   \n",
       "4            NaN      22.116667      18.973333       17.567857   \n",
       "\n",
       "   max_temp_C_365  ...  wind_speed_ms_90  wind_speed_ms_180  \\\n",
       "0       14.515385  ...               NaN                NaN   \n",
       "1       16.564045  ...               NaN                NaN   \n",
       "2       17.354128  ...               NaN                NaN   \n",
       "3       17.461261  ...               NaN                NaN   \n",
       "4       17.567857  ...               NaN                NaN   \n",
       "\n",
       "   wind_speed_ms_365  LATITUDE  LONGITUDE  ELEVATION  STATE  \\\n",
       "0                NaN   31.8814   -86.2503      132.0     AL   \n",
       "1                NaN   31.8814   -86.2503      132.0     AL   \n",
       "2                NaN   31.8814   -86.2503      132.0     AL   \n",
       "3                NaN   31.8814   -86.2503      132.0     AL   \n",
       "4                NaN   31.8814   -86.2503      132.0     AL   \n",
       "\n",
       "             NA_L3NAME                NA_L2NAME                  NA_L1NAME  \n",
       "0  Southeastern Plains  SOUTHEASTERN USA PLAINS  EASTERN TEMPERATE FORESTS  \n",
       "1  Southeastern Plains  SOUTHEASTERN USA PLAINS  EASTERN TEMPERATE FORESTS  \n",
       "2  Southeastern Plains  SOUTHEASTERN USA PLAINS  EASTERN TEMPERATE FORESTS  \n",
       "3  Southeastern Plains  SOUTHEASTERN USA PLAINS  EASTERN TEMPERATE FORESTS  \n",
       "4  Southeastern Plains  SOUTHEASTERN USA PLAINS  EASTERN TEMPERATE FORESTS  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_mon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Summary Data by Date and Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Date', 'max_temp_C', 'min_temp_C', 'precipitation_cm',\n",
       "       'wind_speed_ms', 'max_temp_C_30', 'max_temp_C_90', 'max_temp_C_180',\n",
       "       'max_temp_C_365', 'min_temp_C_30', 'min_temp_C_90', 'min_temp_C_180',\n",
       "       'min_temp_C_365', 'precipitation_cm_30', 'precipitation_cm_90',\n",
       "       'precipitation_cm_180', 'precipitation_cm_365', 'wind_speed_ms_30',\n",
       "       'wind_speed_ms_90', 'wind_speed_ms_180', 'wind_speed_ms_365',\n",
       "       'LATITUDE', 'LONGITUDE', 'ELEVATION', 'STATE', 'NA_L3NAME', 'NA_L2NAME',\n",
       "       'NA_L1NAME'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_mon.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary for aggregate functions\n",
    "f = {'max_temp_C_30': ['mean'], 'max_temp_C_90': ['mean'], 'max_temp_C_180': ['mean'],\n",
    "       'max_temp_C_365': ['mean'], 'min_temp_C_30': ['mean'], 'min_temp_C_90': ['mean'], 'min_temp_C_180': ['mean'],\n",
    "       'min_temp_C_365': ['mean'], 'precipitation_cm_30': ['sum'], 'precipitation_cm_90': ['sum'],\n",
    "       'precipitation_cm_180': ['sum'], 'precipitation_cm_365': ['sum'], 'wind_speed_ms_30': ['mean'],\n",
    "       'wind_speed_ms_90': ['mean'], 'wind_speed_ms_180': ['mean'], 'wind_speed_ms_365': ['mean'],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating average weather statistics for ecoregion level 1\n",
    "level_1_weather = weather_mon.groupby(['Date', 'NA_L1NAME']).agg(f)\n",
    "level_1_weather.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating average weather statistics for ecoregion level 2\n",
    "level_2_weather = weather_mon.groupby(['Date', 'NA_L2NAME']).agg(f)\n",
    "level_2_weather.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating average weather statistics for ecoregion level 3\n",
    "level_3_weather = weather_mon.groupby(['Date', 'NA_L3NAME']).agg(f)\n",
    "level_3_weather.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing CSVs to pull into MTBS analysis\n",
    "level_1_weather.to_csv('Ecoregion Level 1 Weather.csv')\n",
    "level_2_weather.to_csv('Ecoregion Level 2 Weather.csv')\n",
    "level_3_weather.to_csv('Ecoregion Level 3 Weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
